{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model Pipeline   \n",
    "\n",
    "This is the baseline kernel (automatically generated by my bot: Maggle). In this kernel, an end to end classification pipeline is implemented.\n",
    "\n",
    "### Contents \n",
    "\n",
    "1. Prepare Environment  \n",
    "2. Preparation and Exploration   \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 2.1 Dataset Snapshot and Summary    \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 2.2 Target Variable Distribution    \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 2.3 Missing Values    \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 2.4 Variable Correlations\n",
    "3. Preprocessing  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 3.1 Label Encoding    \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 3.2 Missing Values Treatment     \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 3.3 Feature Engineering   \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 3.4 Train Test Split    \n",
    "4. Modelling   \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 4.1 Logistic Regression  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 4.2 Random Forest  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 4.3 Extereme Gradient Boosting  \n",
    "5. Feature Importance   \n",
    "6. Creating Submission  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Prepare Environment\n",
    "Lets load the required libraries to be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from xgboost import plot_importance\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "import seaborn as sns \n",
    "import xgboost as xgb \n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Dataset Preparation\n",
    "Load the train and test dataset into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read dataset\n",
    "train_df = pd.read_csv('../input/train 2.csv')\n",
    "test_df = pd.read_csv(\"../input/test 2.csv\")\n",
    "\n",
    "## get predictor and target variables\n",
    "_target = \"Survived\"\n",
    "_id = \"PassengerId\" \n",
    "\n",
    "_target = \"author\"\n",
    "_id = \"id\" \n",
    "tag = \"text\"\n",
    "\n",
    "Y = train_df[_target]\n",
    "distinct_Y = Y.value_counts().index\n",
    "test_id = test_df[_id]\n",
    "\n",
    "## drop the target and id columns\n",
    "train_df = train_df.drop([_target, _id], axis=1)\n",
    "test_df = test_df.drop([_id], axis=1)\n",
    "\n",
    "textcol = \"text\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Dataset snapshot and summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## snapshot of train and test\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## summary of train and test\n",
    "# if tag != \"text\":\n",
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Target variable distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tar_dist = dict(Counter(Y.values))\n",
    "\n",
    "xx = list(tar_dist.keys())\n",
    "yy = list(tar_dist.values())\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.set(style=\"whitegrid\")\n",
    "ax = sns.barplot(x=xx, y=yy)\n",
    "ax.set_title('Distribution of Target')\n",
    "ax.set_ylabel('count');\n",
    "ax.set_xlabel(_target);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Missing Value Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcount = train_df.isna().sum()\n",
    "xx = mcount.index \n",
    "yy = mcount.values\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.set(style=\"whitegrid\")\n",
    "ax = sns.barplot(x=xx, y=yy)\n",
    "ax.set_title('Number of Missing Values')\n",
    "ax.set_ylabel('Number of Columns');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Variable Correlations \n",
    "\n",
    "Lets plot the correlations among the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = train_df.corr()\n",
    "\n",
    "mask = np.zeros_like(corr, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "plt.figure(figsize=(6,5))\n",
    "f, ax = plt.subplots(figsize=(11, 9))\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0, \n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Data Preprocessing\n",
    "\n",
    "In the data preprocessing step, we will perform label encoding of categorical variables and handle missing values.\n",
    "\n",
    "### 3.1 Label Encoding\n",
    "In this step, convert the categorical variables into label encoded forms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tag != \"text\":\n",
    "    columns = train_df.columns\n",
    "    num_cols = train_df._get_numeric_data().columns\n",
    "    cat_cols = list(set(columns) - set(num_cols))\n",
    "\n",
    "    for col in cat_cols: \n",
    "        le = LabelEncoder()\n",
    "        le.fit(list(train_df[col].values) + list(test_df[col].values))\n",
    "        train_df[col] = le.transform(list(train_df[col].values))\n",
    "        test_df[col] = le.transform(list(test_df[col].values))\n",
    "\n",
    "if Y.dtype.name == \"object\":\n",
    "    le = LabelEncoder()\n",
    "    Y = le.fit_transform(Y.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Missing Values Treatment\n",
    "\n",
    "Handle the missing values, for continuous variables, replace by mean. For categorical variables, replace by mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tag != \"text\":\n",
    "    ## for numerical columns, replace the missing values by mean\n",
    "    train_df[num_cols] = train_df[num_cols].fillna(train_df[num_cols].mean())\n",
    "    test_df[num_cols] = test_df[num_cols].fillna(test_df[num_cols].mean())\n",
    "\n",
    "    ## for categorical columns, replace the missing values by mode\n",
    "    train_df[cat_cols] = train_df[cat_cols].fillna(train_df[cat_cols].mode())\n",
    "    test_df[cat_cols] = test_df[cat_cols].fillna(test_df[cat_cols].mode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Feature Engineering \n",
    "\n",
    "In this section, we will create relevant features which can be used in the modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tag == \"text\":\n",
    "    tfidf = TfidfVectorizer(min_df=3,  max_features=None, analyzer='word', \n",
    "                        token_pattern=r'\\w{1,}', stop_words = 'english')\n",
    "    tfidf.fit(list(train_df[textcol].values))\n",
    "    xtrain =  tfidf.transform(train_df[textcol].values) \n",
    "    xtest =  tfidf.transform(test_df[textcol].values)\n",
    "else:\n",
    "    xtrain = train_df\n",
    "    xtest = test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Train and Validation sets split\n",
    "\n",
    "Create the training and validation sets for training the model and validating it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(xtrain, Y, test_size=0.20, random_state=2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 : Create baseline model\n",
    "\n",
    "Next step is the modelling step, lets start with the simple linear model \n",
    "\n",
    "### 4.1 : Logistic Regression\n",
    "\n",
    "Train a binary classifier logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "valp = model.predict(X_valid)\n",
    "pred = model.predict(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc = roc_auc_score(y_valid, valp)\n",
    "print('AUC:', auc)\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_valid, valp)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.title('Receiver Operating Characteristic for different ML algorithms')\n",
    "plt.plot(fpr, tpr, 'b', label = 'LGBM-AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'upper left')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    plt.figure(figsize=(6,5))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", \n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "cnf_matrix = confusion_matrix(y_valid, valp)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "plot_confusion_matrix(cnf_matrix, classes=distinct_Y, title='Confusion matrix Validation Set')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 : Random Forest Classifier\n",
    "\n",
    "Now, lets train a tree based model : random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "valp = model.predict(X_valid)\n",
    "pred = model.predict(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc = roc_auc_score(y_valid, valp)\n",
    "print('AUC:', auc)\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_valid, valp)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.title('Receiver Operating Characteristic for different ML algorithms')\n",
    "plt.plot(fpr, tpr, 'b', label = 'LGBM-AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'upper left')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnf_matrix = confusion_matrix(y_valid, valp)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "plt.figure(figsize=(6,5));\n",
    "plot_confusion_matrix(cnf_matrix, classes=distinct_Y, title='Confusion matrix Validation Set');\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 : xgBoost Classifier\n",
    "\n",
    "Lets train the extereme gradient boosting : xgboost classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgb.XGBClassifier(n_estimators=300, learning_rate=0.01)\n",
    "model.fit(X_train, y_train)\n",
    "valp = model.predict(X_valid)\n",
    "pred = model.predict(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc = roc_auc_score(y_valid, valp)\n",
    "print('AUC:', auc)\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_valid, valp)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.title('Receiver Operating Characteristic for different ML algorithms')\n",
    "plt.plot(fpr, tpr, 'b', label = 'LGBM-AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'upper left')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnf_matrix = confusion_matrix(y_valid, valp)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "plot_confusion_matrix(cnf_matrix, classes=distinct_Y, title='Confusion matrix Validation Set')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Feature Importance\n",
    "\n",
    "Lets look at some of the important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,5))\n",
    "plot_importance(model, max_num_features=10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6 : Create Submission File\n",
    "\n",
    "Finally, create the submission file from the extereme graident boosting model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.DataFrame()\n",
    "sub[_id] = test_id\n",
    "sub[_target] = pred\n",
    "sub.to_csv(\"baseline_submission.csv\", index=False)\n",
    "sub.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
