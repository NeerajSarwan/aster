{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model Pipeline   \n",
    "\n",
    "Hi, This kernel is automatically generated by the Aster - The kaggle bot to generate baseline kernels for a variety of datasets / competitions. In this kernel, I am using the given dataset for exploration, preprocessing, modelling purposes. Let me walk you through the contents of this kernel:\n",
    "\n",
    "### Contents \n",
    "\n",
    "1. Environment Preparation\n",
    "2. Quick Exploration   \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 2.1 Dataset Preparation   \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 2.2 Dataset Snapshot and Summary    \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 2.3 Target Variable Distribution    \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 2.4 Missing Values    \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 2.5 Variable Types  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 2.6 Variable Correlations\n",
    "3. Preprocessing  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 3.1 Label Encoding    \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 3.2 Missing Values Treatment     \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 3.3 Feature Engineering (text fields)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3.3.1 TF-IDF Vectorizor  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3.3.2 Top Keywords - Wordcloud    \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 3.4 Train Test Split    \n",
    "4. Modelling   \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 4.1 Logistic Regression  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 4.2 Decision Tree    \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 4.3 Random Forest  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 4.4 ExtraTrees Classifier  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 4.5 Extereme Gradient Boosting  \n",
    "5. Feature Importance   \n",
    "6. Model Ensembling  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 6.1 A simple Blender  \n",
    "7. Creating Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Prepare Environment\n",
    "As the first step, lets load all the required libraries to be used in the kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## modelling libraries\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve, auc\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "## preprocessing libraries\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import itertools\n",
    "import os \n",
    "\n",
    "## visualization libraries\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print (\"all libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Quick Exploration\n",
    "In the next step, lets load the dataset into my memory and perform a quick exploratory analysis \n",
    "\n",
    "### 2.1 Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read dataset\n",
    "train_path = \"../input/train.csv\"\n",
    "train_df = pd.read_csv(train_path)\n",
    "train_copy = train_df.copy()\n",
    "\n",
    "test_path = \"../input/test.csv\"\n",
    "test_df = pd.DataFrame()\n",
    "if os.path.exists(test_path):\n",
    "    test_df = pd.read_csv(test_path)\n",
    "\n",
    "print (\"dataset loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## separate predictors and target variables\n",
    "_target = \"author\"\n",
    "Y = train_df[_target]\n",
    "distinct_Y = Y.value_counts().index\n",
    "\n",
    "## separate the id column\n",
    "_id = \"id\"\n",
    "if _id == \"\": ## if id is not present, create a dummy \n",
    "    _id = \"id\"\n",
    "    train_df[_id] = 1\n",
    "    test_df[_id] = 1\n",
    "if _id not in list(test_df.columns):\n",
    "    test_df[_id] = 1\n",
    "    \n",
    "## drop the target and id columns\n",
    "train_df = train_df.drop([_target, _id], axis=1)\n",
    "test_id = test_df[_id]\n",
    "test_df = test_df.drop([_id], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## flag variables (used by bot to write the relevant code)\n",
    "textcol = \"text\"\n",
    "tag = \"doc\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Dataset snapshot and summary\n",
    "\n",
    "Lets look at the dataset snapshot and the summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## snapshot of train and test\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Target variable distribution\n",
    "\n",
    "Lets plot the distribution of target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tar_dist = dict(Counter(Y.values))\n",
    "\n",
    "xx = list(tar_dist.keys())\n",
    "yy = list(tar_dist.values())\n",
    "\n",
    "plt.figure(figsize=(5,3))\n",
    "sns.set(style=\"whitegrid\")\n",
    "ax = sns.barplot(x=xx, y=yy, palette=\"rocket\")\n",
    "ax.set_title('Distribution of Target')\n",
    "ax.set_ylabel('count');\n",
    "ax.set_xlabel(_target);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets generate some plots related to dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tag == \"doc\":\n",
    "    txts = []\n",
    "    for i, y in enumerate(distinct_Y):\n",
    "        txt = \" \".join(train_copy[train_copy[_target] == y][\"text\"]).lower()\n",
    "        txts.append(txt)\n",
    "\n",
    "    for j, text in enumerate(txts):\n",
    "        wc = WordCloud(background_color=\"black\", max_words=2000, stopwords=STOPWORDS)\n",
    "        wc.generate(text)\n",
    "        plt.figure(figsize=(9,8))\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(\"Most frequent words - \" + distinct_Y[j], fontsize=20)\n",
    "        plt.imshow(wc.recolor(colormap= 'cool' , random_state=17), alpha=0.95)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Missing Value Counts \n",
    "\n",
    "Lets check the count of missing values in the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcount = train_df.isna().sum() \n",
    "xx = mcount.index \n",
    "yy = mcount.values\n",
    "\n",
    "missing_cols = 0\n",
    "for each in yy:\n",
    "    if each > 0:\n",
    "        missing_cols += 1\n",
    "print (\"there are \" + str(missing_cols) + \" columns in the dataset having missing values\")\n",
    "\n",
    "if missing_cols > 0:\n",
    "    plt.figure(figsize=(12,5))\n",
    "    sns.set(style=\"whitegrid\")\n",
    "    ax = sns.barplot(x=xx, y=yy, palette=\"gist_rainbow\")\n",
    "    ax.set_title('Number of Missing Values')\n",
    "    ax.set_ylabel('Number of Columns');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Variable Types\n",
    "\n",
    "Lets count the number of numerical and categorical columns in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## find categorical columns in the dataset \n",
    "num_cols = train_df._get_numeric_data().columns\n",
    "cat_cols = list(set(train_df.columns) - set(num_cols))\n",
    "\n",
    "print (\"There are \" + str(len(num_cols)) + \" numerical columns in the dataset\")\n",
    "print (\"There are \" + str(len(cat_cols)) + \" object type columns in the dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Variable Correlations (Only Numerical Fields)\n",
    "\n",
    "Lets plot the correlations among the variables. The generated graph can give an idea about features which are highly, moderately or least correlated with one another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_corr = False\n",
    "corr = train_df.corr()\n",
    "if len(corr) > 0:\n",
    "    get_corr = True\n",
    "    colormap = plt.cm.BrBG\n",
    "    plt.figure(figsize=(10,10));\n",
    "    plt.title('Pearson Correlation of Features', y=1.05, size=15);\n",
    "    sns.heatmap(corr, linewidths=0.1,vmax=1.0, square=True, cmap=colormap, linecolor='white', annot=True);\n",
    "else:\n",
    "    print (\"No variables available for correlation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Data Preprocessing\n",
    "\n",
    "In the data preprocessing step, we will perform label encoding of categorical variables and handle missing values.\n",
    "\n",
    "### 3.1 Label Encoding\n",
    "In this step, convert the categorical variables into label encoded forms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = train_df.columns\n",
    "num_cols = train_df._get_numeric_data().columns\n",
    "cat_cols = list(set(columns) - set(num_cols))\n",
    "    \n",
    "if tag == \"doc\":\n",
    "    print (\"No columns available for label encoding\")\n",
    "elif len(cat_cols) > 0:\n",
    "    for col in cat_cols: \n",
    "        le = LabelEncoder()\n",
    "        \n",
    "        if col in list(test_df.columns):\n",
    "            le.fit(list(train_df[col].values) + list(test_df[col].values))\n",
    "        else:\n",
    "            le.fit(list(train_df[col].values))\n",
    "        \n",
    "        train_df[col] = le.transform(list(train_df[col].values))\n",
    "        try:\n",
    "            test_df[col] = le.transform(list(test_df[col].values))\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "## label encode the target variable (if object type)\n",
    "if Y.dtype.name == \"object\":\n",
    "    le = LabelEncoder()\n",
    "    Y = le.fit_transform(Y.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Missing Values Treatment\n",
    "\n",
    "Handle the missing values, for continuous variables, replace by mean. For categorical variables, replace by mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tag == \"doc\":\n",
    "    train_df[textcol] = train_df[textcol].fillna(\"\")\n",
    "    if textcol in test_df:\n",
    "        test_df[textcol] = test_df[textcol].fillna(\"\")\n",
    "else:\n",
    "    ## for numerical columns, replace the missing values by mean\n",
    "    train_df[num_cols] = train_df[num_cols].fillna(train_df[num_cols].mean())\n",
    "    try:\n",
    "        test_df[num_cols] = test_df[num_cols].fillna(test_df[num_cols].mean())\n",
    "    except:\n",
    "        pass \n",
    "    \n",
    "    ## for categorical columns, replace the missing values by mode\n",
    "    train_df[cat_cols] = train_df[cat_cols].fillna(train_df[cat_cols].mode())    \n",
    "    try:\n",
    "        test_df[cat_cols] = test_df[cat_cols].fillna(test_df[cat_cols].mode())\n",
    "    except:\n",
    "        pass\n",
    "print (\"Treated missing values in the dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Feature Engineering (only for text fields)\n",
    "\n",
    "In this section, we will create relevant features which can be used in the modelling\n",
    "\n",
    "#### 3.3.1 Tf IDF features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tag == \"doc\":\n",
    "    tfidf = TfidfVectorizer(min_df=3,  max_features=None, analyzer='word', \n",
    "                            token_pattern=r'\\w{1,}', stop_words = 'english')\n",
    "    tfidf.fit(list(train_df[textcol].values))\n",
    "    xtrain = tfidf.transform(train_df[textcol].values) \n",
    "    if textcol in test_df.columns:\n",
    "        xtest = tfidf.transform(test_df[textcol].values)\n",
    "else:\n",
    "    xtrain = train_df\n",
    "    xtest = test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tag != \"doc\":\n",
    "    print (\"Lets plot the dataset distributions after preprocessing step ... \")\n",
    "    ## pair plots\n",
    "    sns.pairplot(train_df, palette=\"cool\")\n",
    "    \n",
    "    ## distributions\n",
    "    columns=train_df.columns\n",
    "    plt.subplots(figsize=(18,15))\n",
    "    length=len(columns)\n",
    "    for i,j in itertools.zip_longest(columns,range(length)):\n",
    "        plt.subplot((length/2),3,j+1)\n",
    "        plt.subplots_adjust(wspace=0.2,hspace=0.5)\n",
    "        train_df[i].hist(bins=20, edgecolor='white')\n",
    "        plt.title(i)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Train and Validation sets split\n",
    "\n",
    "Create the training and validation sets for training the model and validating it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(xtrain, Y, test_size=0.20, random_state=2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Train and Validation sets split\n",
    "\n",
    "Create the training and validation sets for training the model and validating it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(xtrain, Y, test_size=0.20, random_state=2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 : Create baseline model\n",
    "\n",
    "Next step is the modelling step, lets start with the simple linear model \n",
    "\n",
    "### 4.1 : Logistic Regression\n",
    "\n",
    "Train a binary classifier logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = LogisticRegression()\n",
    "model1.fit(X_train, y_train)\n",
    "valp = model1.predict(X_valid)\n",
    "\n",
    "def generate_auc(y_valid, valp, model_name):\n",
    "    auc_scr = roc_auc_score(y_valid, valp)\n",
    "    print('The AUC for ' +model_name+ ' is :', auc_scr)\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(y_valid, valp)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    plt.figure(figsize=(6,5))\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.plot(fpr, tpr, 'purple', label = 'AUC = %0.2f' % roc_auc)\n",
    "    plt.legend(loc = 'upper left')\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.show()\n",
    "\n",
    "if len(distinct_Y) == 2:\n",
    "    generate_auc(y_valid, valp, model_name=\"logistic regression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    plt.figure(figsize=(6,5));\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    plt.grid(False)\n",
    "\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", \n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "cnf_matrix = confusion_matrix(y_valid, valp)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "plot_confusion_matrix(cnf_matrix, classes=distinct_Y, title='Confusion matrix Validation Set')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 : Decision Tree Classifier\n",
    "\n",
    "Lets train a decision tree classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = DecisionTreeClassifier()\n",
    "model2.fit(X_train, y_train)\n",
    "valp = model2.predict(X_valid)\n",
    "\n",
    "if len(distinct_Y) == 2:\n",
    "    generate_auc(y_valid,valp, model_name=\"decision tree classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnf_matrix = confusion_matrix(y_valid, valp)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "plt.figure(figsize=(6,5));\n",
    "plot_confusion_matrix(cnf_matrix, classes=distinct_Y, title='Confusion matrix Validation Set');\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 : Random Forest Classifier\n",
    "\n",
    "Now, lets train a tree based model : random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = RandomForestClassifier()\n",
    "model3.fit(X_train, y_train)\n",
    "valp = model3.predict(X_valid)\n",
    "\n",
    "if len(distinct_Y) == 2:\n",
    "    generate_auc(y_valid,valp, model_name=\"random forest classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnf_matrix = confusion_matrix(y_valid, valp)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "plt.figure(figsize=(6,5));\n",
    "plot_confusion_matrix(cnf_matrix, classes=distinct_Y, title='Confusion matrix Validation Set');\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 : ExtraTrees Classifier\n",
    "\n",
    "Now, lets train another tree based model : extra trees classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = ExtraTreesClassifier()\n",
    "model4.fit(X_train, y_train)\n",
    "valp = model4.predict(X_valid)\n",
    "\n",
    "if len(distinct_Y) == 2:\n",
    "    generate_auc(y_valid,valp, model_name=\"extratrees classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnf_matrix = confusion_matrix(y_valid, valp)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "plt.figure(figsize=(6,5));\n",
    "plot_confusion_matrix(cnf_matrix, classes=distinct_Y, title='Confusion matrix Validation Set');\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 : xgBoost Classifier\n",
    "\n",
    "Lets train the extereme gradient boosting : xgboost classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model5 = xgb.XGBClassifier(n_estimators=300, learning_rate=0.01)\n",
    "model5.fit(X_train, y_train)\n",
    "valp = model5.predict(X_valid)\n",
    "\n",
    "if len(distinct_Y) == 2:\n",
    "    generate_auc(y_valid,valp, model_name=\"xgboost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnf_matrix = confusion_matrix(y_valid, valp)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "plot_confusion_matrix(cnf_matrix, classes=distinct_Y, title='Confusion matrix Validation Set')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Feature Importance\n",
    "\n",
    "Lets look at some of the important features from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "xgb.plot_importance(model5, max_num_features=10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6 : Model Ensembling\n",
    "\n",
    "Lets create a simple blender. Other options to extend are stacking / majority voting / rank averaging etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [model1, model2, model3, model4, model5]\n",
    "preds = np.zeros(shape=(xtest.shape[0],))\n",
    "try:\n",
    "    for model in models:\n",
    "        pred = model.predict(xtest)/ len(models)\n",
    "        preds += pred\n",
    "    print (preds[:100])\n",
    "except:\n",
    "    print (\"this is a dataset kernel, no test data for predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7 : Create Submission File\n",
    "\n",
    "Finally, create the submission file from the extereme graident boosting model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    pred = model5.predict(xtest)\n",
    "    sub = pd.DataFrame()\n",
    "    sub[_id] = test_id\n",
    "    sub[_target] = pred\n",
    "    sub.to_csv(\"baseline_submission.csv\", index=False)\n",
    "    print (\"Submission File Generated, here is the snapshot: \")\n",
    "    print (sub.head(10))\n",
    "except:\n",
    "    print (\"This is a dataset kernel, no need to create a submission file :)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks for viewing this kernel, hopefully you can get ideas to start your own kernel."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
