{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model Pipeline   \n",
    "\n",
    "This is the baseline kernel (automatically generated by my bot: Maggle). In this kernel, an end to end classification pipeline is implemented.\n",
    "\n",
    "### Contents \n",
    "\n",
    "1. Prepare Environment  \n",
    "2. Preparation and Exploration   \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 2.1 Dataset Snapshot and Summary    \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 2.2 Target Variable Distribution    \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 2.3 Missing Values    \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 2.4 Variable Correlations\n",
    "3. Preprocessing  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 3.1 Label Encoding    \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 3.2 Missing Values Treatment     \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 3.3 Feature Engineering   \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 3.4 Train Test Split    \n",
    "4. Modelling   \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 4.1 Logistic Regression  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 4.2 Random Forest  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; 4.3 Extereme Gradient Boosting  \n",
    "5. Feature Importance   \n",
    "6. Creating Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Prepare Environment\n",
    "Lets load the required libraries to be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from xgboost import plot_importance\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "import seaborn as sns \n",
    "import xgboost as xgb \n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Dataset Preparation and Exploration\n",
    "Load the train and test dataset into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read dataset\n",
    "train_df = pd.read_csv('../input/train 2.csv')\n",
    "test_df = pd.read_csv(\"../input/test 2.csv\")\n",
    "\n",
    "## get predictor and target variables\n",
    "_target = \"Survived\"\n",
    "_id = \"PassengerId\" \n",
    "\n",
    "_target = \"author\"\n",
    "_id = \"id\" \n",
    "tag = \"text\"\n",
    "\n",
    "Y = train_df[_target]\n",
    "distinct_Y = Y.value_counts().index\n",
    "test_id = test_df[_id]\n",
    "\n",
    "## drop the target and id columns\n",
    "train_df = train_df.drop([_target, _id], axis=1)\n",
    "test_df = test_df.drop([_id], axis=1)\n",
    "\n",
    "textcol = \"text\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Dataset snapshot and summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## snapshot of train and test\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## summary of train and test\n",
    "# if tag != \"text\":\n",
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Target variable distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tar_dist = dict(Counter(Y.values))\n",
    "\n",
    "xx = list(tar_dist.keys())\n",
    "yy = list(tar_dist.values())\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.set(style=\"whitegrid\")\n",
    "ax = sns.barplot(x=xx, y=yy)\n",
    "ax.set_title('Distribution of Target')\n",
    "ax.set_ylabel('count');\n",
    "ax.set_xlabel(_target);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Missing Value Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcount = train_df.isna().sum()\n",
    "xx = mcount.index \n",
    "yy = mcount.values\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.set(style=\"whitegrid\")\n",
    "ax = sns.barplot(x=xx, y=yy)\n",
    "ax.set_title('Number of Missing Values')\n",
    "ax.set_ylabel('Number of Columns');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Variable Correlations \n",
    "\n",
    "Lets plot the correlations among the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = train_df.corr()\n",
    "\n",
    "mask = np.zeros_like(corr, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "plt.figure(figsize=(6,5))\n",
    "f, ax = plt.subplots(figsize=(11, 9))\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0, \n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5});"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
